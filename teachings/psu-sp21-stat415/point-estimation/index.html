<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title></title>
    <base href="/">
    <meta name="description" content="A simple monospaced resume theme for Hugo.">
    <meta name="author" content='Hyebin Song'><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    
    <link rel="stylesheet" href="/sass/researcher.min.css">

    
        <link rel="icon" type="image/ico" href="/favicon.ico">
    
</head>

    <body><div class="container mt-5">
    <nav class="navbar navbar-expand-sm flex-column flex-sm-row p-0">
        <a class="navbar-brand mr-sm-auto" href="/">Hyebin Song</a>
        <div class="navbar-nav flex-row">
            
                
                
                    <a class="nav-item nav-link" href="/about">About</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/publications">Publications</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/teaching">Teaching</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/software">Software</a>
                    
                        <span class="nav-item navbar-text mx-1">/</span>
                    
                
                    <a class="nav-item nav-link" href="/cv_hsong.pdf">CV</a>
                    
                
            
        </div>
    </nav>
</div>
<hr>
<div id="content">
<div class="container">
    <p><em>The Pennsylvania State University, Spring 2021 Stat 415-001, Hyebin Song</em></p>
<h1 id="point-estimation">Point Estimation</h1>
<p><a href="/teachings/psu-sp21-stat415/main.html">Go to course main page</a></p>
<p>[TOC]</p>
<h2 id="introduction-to-point-estimation">Introduction to Point Estimation</h2>
<h3 id="learning-objectives">Learning objectives</h3>
<ol>
<li>Understand the goal of point estimation</li>
<li>Understand bias and mean square error of point estimators</li>
</ol>
<h3 id="recap">Recap</h3>
<p><strong>Setting:</strong> $X_1,\dots,X_n\sim F_\theta$, i,i,d.,  where $\theta$ is an unknown value in the parameter space $\Omega$.</p>
<ul>
<li>we have a sample $(X_1,\dots,X_n)$ where $X_i$ is independent and identically distributed (i.i.d), and the distribution of $X_i$ is known up to the parameter value. That is, we know that the distribution function of $X_i$ is in the collection of distribution functions ${F_\theta; \theta \in \Omega}$, but we cannot pick one because we do not know the value of $\theta$.</li>
<li>the goal is to choose a &ldquo;plausible&rdquo; $\theta$ in $\Omega$ based on the sample $(X_1,\dots,X_n)$.</li>
</ul>
<p><strong>Recall the definitions:</strong></p>
<p><img src="Figures/point estimation.png" alt="point estimation" style="zoom: 67%;" /></p>
<ol>
<li>A point estimator of $\theta$:</li>
<li>A point estimate of $\theta$:</li>
</ol>
<p><strong>Notation:</strong></p>
<p>Recall that for a continuous random variable $X$ with a pdf $f_\theta$, probability of an event ${X\in B}$, expectation and variance of $X$ are computed as</p>
<ul>
<li>
<p>$P(X\in B) = \int_B f_\theta(x)dx$</p>
</li>
<li>
<p>$E<input checked="" disabled="" type="checkbox"> = \mu= \int x f_\theta(x)dx$</p>
</li>
<li>
<p>${\rm Var}<input checked="" disabled="" type="checkbox"> = \int (x-\mu)^2 f_\theta(x) dx$</p>
</li>
</ul>
<blockquote>
<p>Note: for a discrete random variable $X$, we can replace a pdf with a pmf and an integration with a summation.</p>
</blockquote>
<p>We sometimes use subscripts and write $P_\theta(X\in B)$, $E_\theta[X]$ and ${\rm Var}_\theta[X]$ to emphasize that expectation and variance are computed using a pdf with the parameter $\theta$.</p>
<h3 id="the-bias-and-mean-square-error-of-point-estimators">The bias and mean square error of point estimators</h3>
<p><strong>Definition (Bias and unbiased estimator)</strong></p>
<ul>
<li>${\rm Bias}(\widehat{\theta}; \theta) = E_\theta[\widehat{\theta}(X_1,\dots,X_n)]-\theta$</li>
<li>If ${\rm Bias}(\widehat{\theta}; \theta$) $=0$ for all $\theta \in \Omega$,  then $\widehat{\theta}(X_1,\dots,X_n)$ is an <strong>unbiased</strong> estimator. Otherwise, the estimator is <strong>biased</strong>.</li>
</ul>
<p><img src="Figures/unbiased.png" style="zoom: 50%;" /></p>
<p><strong>Mean Squared Errors (MSE)</strong></p>
<ul>
<li>
<p><strong>Definition</strong>: ${\rm MSE}(\widehat{\theta}; \theta) = E_\theta[(\widehat{\theta}(X_1,\dots,X_n)-\theta)^2]$</p>
</li>
<li>
<p>MSE captures both bias and variance</p>
<p>${\rm MSE}(\widehat{\theta};\theta) =  {\rm Var}_\theta(\widehat{\theta})+{\rm Bias}(\widehat{\theta};\theta)^2$</p>
<blockquote>
<p>Proof. $E_\theta[(\widehat{\theta}-\theta)^2] = E_\theta[(\widehat{\theta}-E_\theta(\hat{\theta})+E_\theta(\hat{\theta})-\theta)^2]=E_\theta[(\widehat{\theta}-E_\theta(\hat{\theta}))^2]+(E_\theta(\hat{\theta})-\theta)^2$</p>
</blockquote>
</li>
<li>
<p>In particular, if an estimator $\widehat{\theta}$ is unbiased, ${\rm MSE}(\widehat{\theta};\theta) = {\rm Var}_\theta(\widehat{\theta})$.</p>
</li>
</ul>
<h2 id="method-of-moments-estimation">Method of Moments Estimation</h2>
<h3 id="learning-objectives-1">Learning objectives</h3>
<ol>
<li>Understand how to compute method of moments estimators</li>
</ol>
<p><strong>Setting:</strong> $X_1,\dots,X_n\sim F_\theta$, i,i,d.,  where $\theta$ is an unknown value in the parameter space $\Omega$.</p>
<ul>
<li>
<p>Recall the $k$th (population) moment of $X$ is defined as $\mu_k = E[X^k]$.</p>
</li>
<li>
<p>Define the $k$th (sample) moment of a sample $(X_1,\dots,X_n)$  as $M_k = E[X^k]$.</p>
</li>
</ul>
<p><strong>Idea:</strong> Substitution principle</p>
<ul>
<li>
<p>From WLLN, we know $\frac{1}{n}\sum_{i=1}^n X_i^k \approx \mu_k$ for $k=1,2,3,\dots,$</p>
</li>
<li>
<p>If $\theta = g(\mu_1,\dots,\mu_p)$,  then $\widehat{\theta}^{MoM}(X_1,\dots,X_n) = g(M_1,\dots,M_p)$</p>
</li>
</ul>
<p><strong>Example.</strong> $X_1,\dots,X_n\sim N(\mu,1)$, i,i,d. We want to estimate $\mu$.</p>
<blockquote>
<p>sol&gt; $\theta = \mu$.  $\widehat{\theta}^{MoM}(X_1,\dots,X_n) =  \frac{1}{n}\sum_{i=1}^n X_i$.</p>
</blockquote>
<h3 id="procedures-to-obtain-method-of-moments-mom-estimators-of-boldsymboltheta--theta_1dotstheta_p">Procedures to obtain Method of Moments (MoM) estimators of $\boldsymbol{\theta} = (\theta_1,\dots,\theta_p)$</h3>
<p>Population moments of $X$ $\mu_1,\dots,\mu_k$ can be represented as functions of $\boldsymbol{\theta} = (\theta_1,\dots,\theta_p)$</p>
<ol>
<li>Write $\mu_1, \dots, \mu_p$ as functions of $\theta_1,\dots,\theta_p$.</li>
</ol>
<p>$$
\begin{align}
\mu_1 &amp;= h_1(\theta_1,\dots,\theta_p)\<br>
\mu_2 &amp;= h_2(\theta_1,\dots,\theta_p)\<br>
&amp;\dots\<br>
\mu_p &amp;= h_p(\theta_1,\dots,\theta_p)  \tag{1}
\end{align}
$$</p>
<ol start="2">
<li>Solve (1) with respect to $\theta_1,\dots,\theta_p$. That is, find $g_1,g_2,\dots,g_p$ such that</li>
</ol>
<p>$$
\begin{align}
\theta_1 &amp;= h_1(\mu_1,\dots,\mu_p)\<br>
\theta_2 &amp;= h_2(\mu_1,\dots,\mu_p)\<br>
&amp;\dots\<br>
\theta_p &amp;= h_p(\mu_1,\dots,\mu_p)<br>
\end{align}
$$</p>
<ol start="3">
<li>Substitued population moments with sample moments</li>
</ol>
<p>$$
\begin{align}
\widehat{\theta}_1^{MoM} &amp;= h_1(M_1,\dots,M_p)\<br>
\widehat{\theta}_2^{MoM} &amp;= h_2(M_1,\dots,M_p)\<br>
&amp;\dots\<br>
\widehat{\theta}_p^{MoM} &amp;= h_p(M_1,\dots,M_p)<br>
\end{align}
$$</p>
<p><strong>Example</strong> $X_1,\dots,X_n\sim N(\mu,\sigma^2)$, i,i,d. We want to estimate $\boldsymbol{\theta}=[\mu,\sigma^2]$.</p>
<blockquote>
<p>sol&gt;</p>
<ol>
<li>
<p>Write $\mu_1,\mu_2$ as functions of $\theta_1,\theta_2$.</p>
</li>
<li>
<p>$\mu_1 = \mu = \theta_1$</p>
</li>
<li>
<p>$\mu_2 = \sigma^2 + \mu^2 = \theta_2+\theta_1^2$.</p>
</li>
<li>
<p>Solve 1 with respect to $\theta_1,\theta_2$.</p>
<ol>
<li>$\theta_1= \mu_1$</li>
<li>$\theta_2 = \mu_2 - \mu_1^2$.</li>
</ol>
</li>
<li>
<p>Substitued population moments with sample moments</p>
<ol>
<li>$\widehat{\theta}_1^{MoM} = M_1$</li>
<li>$\widehat{\theta}_2^{MoM} = M_2 - M_1^2$</li>
</ol>
</li>
</ol>
<p>Therefore,</p>
<ol>
<li>$\widehat{\theta}_1^{MoM} = \bar{X}$</li>
<li>$\widehat{\theta}<em>2^{MoM} = \frac{1}{n}\sum</em>{i=1}^n X_i^2 - \bar{X}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X})^2$</li>
</ol>
</blockquote>
<p><strong>Remark:</strong> Method of Moment estimator for $\mu$ is unbiased, but Method of Moment estimator for $\sigma^2$ is unbiased</p>
<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<h3 id="learning-objectives-2">Learning objectives</h3>
<ol>
<li>Understand how to compute maximum likelihood estimators (MLE)</li>
<li>Understand invariance property of MLE</li>
</ol>
<p><strong>Setting:</strong> $X_1,\dots,X_n\sim F_\theta$, i,i,d.,  where $\theta$ is an unknown value in the parameter space $\Omega$.</p>
<ul>
<li>Recall the $k$th (population) moment of $X$ is defined as $\mu_k = E[X^k]$.</li>
</ul>
<p><strong>Idea:</strong> Choose the value of $\theta \in \Omega$ that is <strong>most likely</strong> to have given rise to the observed data</p>
<p><strong>Example:</strong> Suppose $X_1,\dots,X_n\sim {\rm Ber}(p)$. We have the observed sample $(1,1,0,1)$. Parameter space $\Omega = {0.2, 0.7}$.</p>
<p>Given $p \in \Omega$, what is the probability of observing $(1,1,0,1)$?</p>
<blockquote>
<ol>
<li>when $p=0.2$, $P_{p=0.2}(X_1=1,X_2=1,X_3=0,X_4=1) = (0.2)^3 (0.8)$</li>
<li>when $p=0.7$, $P_{p=0.7}(X_1=1,X_2=1,X_3=0,X_4=1) = (0.7)^3 (0.2)$</li>
</ol>
<p>Thus, $p=0.7$ is the maximum likelihood estimate of $p$.</p>
</blockquote>
<p><strong>Remark:</strong></p>
<ol>
<li>For any $p \in [0,1]$, $P_{p}(X_1=1,X_2=1,X_3=0,X_4=1) = p^3 (1-p)$ is a function of $p$.</li>
<li>Finding a maximum likelihood estimate can be viewed as finding a maximizer of the function $L(p) = p^3(1-p)$ for $p = 0.2,0.7$.</li>
<li>We call $L(p; 1,1,0,1) = p^3(1-p)$ as the <strong>likelihood function</strong>.</li>
</ol>
<p><strong>Definition (Likelihood function)</strong></p>
<ul>
<li>
<p>The likelihood function of $\boldsymbol{\theta} = [ \theta_1,\dots,\theta_p]$ based on the observed sample $(x_1,\dots,x_n)$
$$
\begin{align}
L(\theta_1,\dots,\theta_p; x_1,\dots,x_n)
&amp; = \prod_{i=1}^n p(x_i; \theta_1,\dots,\theta_p) \mbox{  (discrete $X_i$)}\<br>
&amp; = \prod_{i=1}^n f(x_i; \theta_1,\dots,\theta_p) \mbox{  (continuous $X_i$)}\<br>
\end{align}
$$</p>
</li>
<li>
<p>The log likelihood function of $\boldsymbol{\theta} = [ \theta_1,\dots,\theta_p]$ based on the observed sample $(x_1,\dots,x_n)$</p>
<p>$\log L(\theta_1,\dots,\theta_p; x_1,\dots,x_n) $.</p>
</li>
</ul>
<p><strong>Definition (Maximum Likelihood Estimator)</strong></p>
<ul>
<li>
<p>The maximum likelihood estimate is a value  $\boldsymbol{\theta} \in \Omega$ which maximizes the likelihood function  $L(\theta_1,\dots,\theta_p; x_1,\dots,x_n) $. In other words,</p>
<p>$\hat{\boldsymbol{\theta}}(x_1,\dots,x_n) = (\hat{\theta_1},\dots,\hat{\theta_p})$ such that</p>
<p>$L(\hat{\theta_1},\dots,\hat{\theta_p}; x_1,\dots,x_n)\geq L(\theta_1,\dots,\theta_p; x_1,\dots,x_n) $, for any $\boldsymbol{\theta} \in \Omega$.</p>
</li>
<li>
<p>The maximum likelihood estimator is $\hat{\boldsymbol{\theta}}(X_1,\dots,X_n)$.</p>
</li>
<li>
<p>Since $\log(\cdot)$ is a strictly increasing function, the maximizer of a likelihood $L(\theta_1,\dots,\theta_p; x_1,\dots,x_n) $ is the same as the maximizer of the log-likelihood $\log L(\theta_1,\dots,\theta_p; x_1,\dots,x_n) $. Maximizing the log-likelihood is often easier.</p>
</li>
</ul>
<p><strong>Example</strong>. Likelihood and log-likelihood function of $p$ based on the observed sample $(x_1,\dots,x_n)$, when $X_i \sim {\rm Ber}(p)$ i.i.d., $\Omega= [0,1]$.</p>
<blockquote>
<p>$p(x_i ;p) = p^{x_i} (1-p)^{1-x_i}$.</p>
<p>Therefore,</p>
<ul>
<li>
<p>$L(p;x_1,\dots,x_n) = \prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} = p^{\sum_{i=1}^n x_i} (1-p)^{n-\sum_{i=1}^n x_i}$</p>
</li>
<li>
<p>$\log L(p;x_1,\dots,x_n) = {\sum_{i=1}^n x_i} \log p + (n-{\sum_{i=1}^n x_i})\log (1-p) $</p>
</li>
</ul>
</blockquote>
<h3 id="procedures-to-obtain-maximum-likelihood-estimators-mle-of-boldsymboltheta--theta_1dotstheta_p">Procedures to obtain Maximum Likelihood Estimators (MLE) of $\boldsymbol{\theta} = (\theta_1,\dots,\theta_p)$</h3>
<ol>
<li>
<p>Compute the log-likelihood funciton $\log L(\theta_1,\dots,\theta_p; x_1,\dots,x_n)$</p>
<ol>
<li>Find the pdf or pmf of $X_i$</li>
<li>Find the log-likelihood function
$$
\begin{align}
\log L(\theta_1,\dots,\theta_p; x_1,\dots,x_n)
&amp;= \sum_{i=1}^n \log p(x_i; \theta_1,\dots,\theta_p) \mbox{ (if discrete) }\<br>
&amp;= \sum_{i=1}^n \log f(x_i; \theta_1,\dots,\theta_p) \mbox{ (if continuous) }
\end{align}
$$</li>
</ol>
</li>
<li>
<p>Find a maximizer $\hat{\boldsymbol{\theta}}(x_1,\dots,x_n) = (\hat{\theta_1},\dots,\hat{\theta_p})$ of the log-likelihood function $\log L(\theta_1,\dots,\theta_p; x_1,\dots,x_n)$</p>
<ol>
<li>Compute stationary points of the log-likelihood.
<ul>
<li>When the likelihood is differentiable (most cases), find the solution of the equation $\triangledown \log L(\theta_1,\dots,\theta_p; x_1,\dots,x_n) =0$ inside the parameter space.</li>
</ul>
</li>
<li>Find a maximizer among candidate points (stationary points and boundary points).</li>
</ol>
</li>
<li>
<p>Replace the observed sample $(x_1,\dots,x_n)$ with a random sample $(X_1,\dots,X_n)$ to obtain the maximum likelihood estimator $\hat{\boldsymbol{\theta}}(X_1,\dots,X_n)$.</p>
</li>
</ol>
<p>**Example (Bernoulli MLE) ** Compute the MLE of $p$ for a sample $(X_1,\dots,X_n)$, when $X_i \sim {\rm Ber}(p)$ i.i.d., $\Omega= (0,1)$.</p>
<blockquote>
<ol>
<li>
<p>$\log L(p;x_1,\dots,x_n) = {\sum_{i=1}^n x_i} \log p + (n-{\sum_{i=1}^n x_i})\log (1-p) $</p>
</li>
<li>
<p>a. Find stationary points in $(0,1)$.</p>
<p>$\frac{d}{dp} \log L(p;x_1,\dots,x_n) = {\sum_{i=1}^n x_i} \frac{1}{p}- (n-{\sum_{i=1}^n x_i})\frac{1}{1-p}=0$</p>
<p>Solving for $p$, we get the solution of $\hat{p} = \bar{x}$ .</p>
<p>When $0&lt;\sum_{i=1}^n x_i&lt;n$, $\bar{x}$ is the unique stationary point in $(0,1)$.</p>
<p>When $\sum_{i=1}^n x_i=0$ or $n$, there exist no stationary points in $(0,1)$.</p>
<p>b. When $0&lt;\sum_{i=1}^n x_i&lt;n$,  $\hat{p} = \bar{x}$  is the global maximizer, since</p>
<p>$\frac{d^2}{dp^2} \log L(p;x_1,\dots,x_n) = -{\sum_{i=1}^n x_i} \frac{1}{p^2}- (n-{\sum_{i=1}^n x_i})\frac{1}{(1-p)^2}&lt;0$, for $0&lt;p&lt;1$, and</p>
<p>$\lim_{p\rightarrow \pm\infty}\log L(p;x_1,\dots,x_n) = -\infty$.</p>
<p>When $\sum_{i=1}^n x_i=0$ or $n$,</p>
</li>
</ol>
<p>$$
\log L(p;x_1,\dots,x_n) =\begin{cases}
n \log (1-p) &amp; {\rm if},, \sum_{i=1}^n x_i = 0\<br>
n\log  p &amp; {\rm if},,\sum_{i=1}^n x_i = n\end{cases}
$$</p>
<p>and it is straightforward to verify that both functions are monotone and the maximum achieves at $p=0$ and $p=1$. Then again, $\hat{p} = \bar{x}$  is the global   maximizer.</p>
<p>Therefore, the maximum likelihood estimate of $p$:  $\hat{p}(x_1,\dots,x_n) = \bar{x}$.</p>
<ol start="3">
<li>The maximum likelihood estimator of $p$:   $\hat{p}(X_1,\dots,X_n) = \bar{X}$.</li>
</ol>
</blockquote>
<p><strong>Example (Normal MLE, both $\mu$  and $\sigma^2 $ unknown)</strong> Compute the MLE of $p$ for a sample $(X_1,\dots,X_n)$, when $X_i \sim N(\theta_1,\theta_2)$ i.i.d., $\Omega= {(\theta_1,\theta_2); -\infty&lt;\theta_1&lt;\infty, 0&lt;\theta_2&lt;\infty}$</p>
<blockquote>
<p>We have $\boldsymbol{\theta} = [\theta_1,\theta_2]$</p>
<ol>
<li>
<p>a. pdf of $X_i$: $f(x_i; \boldsymbol{\theta}) = \frac{1}{\sqrt{2\pi\theta_2}}e^{-\frac{1}{2\theta_2}(x_i-\theta_1)^2}$</p>
<p>b. Log-likelihood $\log L(\boldsymbol{\theta}; x_1,\dots,x_n) = \sum_{i=1}^n { -\frac{1}{2}\log (2\pi)-\frac{1}{2}\log(\theta_2)-\frac{1}{2\theta_2} (x_i-\theta_1)^2 }$</p>
</li>
<li>
<p>a. Find stationary points in $\Omega$</p>
<p>$$
\triangledown \log L(\boldsymbol{\theta}; x_1,\dots,x_n) =
\begin{bmatrix}<br>
\frac{1}{\theta_2}\sum_{i=1}^n (x_i-\theta_1)\<br>
-\frac{n}{2\theta_2} +\sum_{i=1}^n\frac{1}{2\theta_2^2} (x_i-\theta_1)^2 \end{bmatrix}
=0
$$
we get $\hat{\theta}_1 = \bar{x}$, $\hat{\theta}_2 = \frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2$.</p>
<p>b. we can verify the solution in a is indeed the unique global maximizer by using a second derivative condition (positive determinant and negative first element of the hessian matrix) and checking that there is no maximum at infinity.</p>
</li>
<li>
<p>The maximum likelihood estimator $\boldsymbol{\theta} = [\bar{X}, n^{-1}\sum_{i=1}^n(X_i-\bar{X})^2]$</p>
</li>
</ol>
</blockquote>
<p>**Theorem (Invariance) ** If $\widehat{\theta}$ is the MLE of $\theta$, then for any function $g$, $g(\widehat{\theta})$ is the MLE of $g(\theta)$.</p>
<p><strong>Remark1</strong> Theorem 6.4-1 in HTZ requires additional condition that $g$ is one-to-one. Although we have a simple proof when $g$ is one-to-one, the statement is true without the condition on $g$.</p>
<blockquote>
<p>Proof (when $g$ is one-to-one)</p>
<p>Let $\eta = g(\theta)$.  We have that $\widehat{\theta}$ is the maximizer of $L(\theta; x_1,\dots,x_n) = L(g^{-1}(\eta);x_1,\dots,x_n)$.</p>
<p>This function has the largest value  $L(\widehat{\theta}; x_1,\dots,x_n) = L(g^{-1}(g(\widehat{\theta})); x_1,\dots,x_n)$.</p>
<p>Therefore, $\hat{\eta}  = g(\hat{\theta})$ is the maximizer of the function $L(g^{-1}(\eta);x_1,\dots,x_n)$.</p>
</blockquote>
<p>**Example (Bernoulli MLE) ** Compute the MLE of ${\rm Var}(X_i)$ for a sample $(X_1,\dots,X_n)$, when $X_i \sim {\rm Ber}(p)$ i.i.d., $\Omega= (0,1)$.</p>
<blockquote>
<p>The parameter of interest $\eta = {\rm Var}(X_i) = p(1-p)$.</p>
<p>Since the MLE of $p$ is $\bar{X}$, the MLE of $\eta$ is $\bar{X}(1-\bar{X})$.</p>
</blockquote>
<h2 id="properties-of-point-estimators">Properties of Point Estimators</h2>
<h3 id="learning-objectives-3">Learning objectives</h3>
<ol>
<li>Understand  various finite sample (unbiasedness, sufficiency) and large $n$ properties (consistency, asymptotic normality/efficiency) of point estimators</li>
<li>Understand optimal properties of maximum likelihood estimators</li>
</ol>
<h3 id="finite-sample-properties">Finite sample properties</h3>
<ul>
<li>
<p><strong>Unbiasedness</strong></p>
<ul>
<li>$\widehat{\theta}$ is an unbiased estimator if ${\rm Bias}(\widehat{\theta}; \theta$) $=0$ for all $\theta \in \Omega$.</li>
</ul>
<p>**Example: ** for a sample $(X_1,\dots,X_n)$, when $X_i \sim {\rm Poisson}(\lambda)$, i.i.d., both $\bar{X}$ and $S^2$ are unbiased.</p>
<p>Among unbiased estimators $\widehat{\theta}_1$ and $\widehat{\theta}_2$, which estimator should we use?</p>
</li>
<li>
<p><strong>Relative Efficiency</strong></p>
<ul>
<li><strong>Definition</strong>: Given two unbiased estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ of a parameter $\theta$, the efficiency of $\hat{\theta}_1$ relative to $\hat{\theta}_2$ , denoted ${\rm eff}( \hat{\theta}_1,\hat{\theta}_2 )$, is defined to be the ratio</li>
</ul>
<p>$$
{\rm eff}( \hat{\theta}_1,\hat{\theta}<em>2 ) = \frac{{\rm Var}</em>\theta(\hat{\theta}<em>2)}{{\rm Var}</em>\theta(\hat{\theta}_1)}
$$</p>
<p>​		$\hat{\theta}_1$ is more efficient than $\hat{\theta}_2$ iff ${\rm eff}( \hat{\theta}_1,\hat{\theta}_2 )&gt;1$. Obviously, given any two unbiased estimators, we would prefer a more efficent estimator.</p>
<p>​		Pushing this idea even forward, we can consider the &ldquo;best&rdquo; unbiased estimator, which we define in the following way.</p>
</li>
<li>
<p><strong>Definition (Minimum Variance Unbiased Estimator (MVUE))</strong> An estimator $\hat{\theta}$ is a best unbiased estimator of $\theta$ if $\hat{\theta}$ is unbiased (i.e., $E_\theta[\hat{\theta}] = \theta$, for all $\theta \in \Omega$), and for any other unbiased estimator $T$, ${\rm Var}_\theta(\hat{\theta}) \leq {\rm Var}_\theta(T)$, for all $\theta \in \Omega$.</p>
<p>Finding a best unbiased estimator (if it exists!) is not an easy task for a variety of reasons. One obvious challenge is that we cannot check the variances of all unbiased estimators. The following theorem provides a lower bound on the variance of <strong>any</strong> unbiased estimator of $\theta$. That is, if we find an unbiased estimator of which variance matches with the lower bound, then we know that we have found the MVUE.</p>
<ul>
<li><strong>Theorem (Cramer-Rao Inequality)</strong> $X_1,\dots,X_n\sim F_\theta$, i,i,d. with pdf (or pmf) $f_\theta(x)$. Let $\hat{\theta}(X_1,\dots,X_n)$ be any unbiased estimator of $\theta$. Under very general conditions,</li>
</ul>
<p>$$
{\rm Var}<em>\theta(\hat{\theta}) \geq \frac{1}{I(\theta)}, ,, \mbox{where } I(\theta)= nE</em>\theta[-\frac{\partial^2 \log f_\theta(X)}{\partial \theta^2}]
$$</p>
<ul>
<li>We say an unbiased estimator $\hat{\theta}$ is <strong>efficient</strong> if the variance of the $\hat{\theta}$ equals to the Cramer-Rao lower bound $I(\theta)^{-1}$.</li>
</ul>
</li>
<li>
<p><strong>Sufficiency</strong></p>
<ul>
<li>An statistic $T(X_1,\dots,X_n)$ is said to be <strong>sufficient</strong> for a parameter $\theta$ if the estimator is in a sense summarize all the information in a sample about a target parameter $\theta$ (the formal definition will be presented later).</li>
<li>Often, &ldquo;good&rdquo; estimators are functions of a sufficient statistic.</li>
</ul>
</li>
</ul>
<h3 id="large-sample-asymptotic-properties">Large sample (asymptotic) properties</h3>
<ul>
<li>
<p><strong>Consistency</strong>: If $P_\theta(|\widehat{\theta}(X_1,\dots,X_n) - \theta|\geq \epsilon) \underset{n\rightarrow \infty}{\rightarrow} 0, \forall \epsilon&gt;0$, then $\widehat{\theta}=\widehat{\theta}(X_1,\dots,X_n)$ is a consistent estimator.</p>
<p><strong>Remark:</strong> consistency is often considered as a &ldquo;minimum requirement&rdquo; an estimator should meet.</p>
</li>
<li>
<p><strong>Asymptotic Normality</strong>: we say an estimator $\widehat{\theta}$ is asymptotically normal if the distribution of $\widehat{\theta}$ $\approx  N(\theta, V(\theta))$ when $n$ is large.</p>
<p><strong>Example:</strong> for a sample $(X_1,\dots,X_n)$, when $X_i \sim {\rm Ber}(p)$, is $\bar{X}$ an asymptotically normal estimator of $p$?</p>
<blockquote>
<p>$\bar{X} \overset{\cdot}{\sim} N(p, \frac{p(1-p)}{n})$ by CLT. Thus $\bar{X}$ is an asymptotically normal estimator of $p$.</p>
</blockquote>
</li>
<li>
<p><strong>Asymptotic Efficiency</strong>: we say an asymptotically normal estimator $\widehat{\theta}$ is asymptotically efficient if $V(\theta)$ is the Cramer-Rao Lower bound $1/I(\theta)$.</p>
<p><strong>Example:</strong> for a sample $(X_1,\dots,X_n)$, when $X_i \sim {\rm Ber}(p)$, is $\bar{X}$ an asymptotically efficient estimator of $p$?</p>
<blockquote>
<p>We have, $\log p_\theta(X) = X\log p+(1-X)\log (1-p)$, and</p>
<p>$$E[\frac{\partial^2}{\partial p^2}\log p_\theta(X)] = -E[\frac{X}{p^2}+\frac{(1-X)}{(1-p)^2}] = -\frac{1}{p}-\frac{1}{1-p}$$.</p>
<p>Therefore, $I(p) = n(\frac{1}{p}+\frac{1}{1-p}) = \frac{n}{p(1-p)}$.</p>
<p>Since $V(p) = \frac{p(1-p)}{n} = I(p)^{-1}$, $\bar{X}$ is an asymptotically efficient estimator.</p>
</blockquote>
</li>
</ul>
<h3 id="properties-of-mom-estimators">Properties of MoM estimators</h3>
<ul>
<li>In most practical cases, MoM estimators are consistent.</li>
<li>Not necessarily unbiased.</li>
<li>Often, MoM estimators are not functions of sufficient statistics.</li>
<li>We may often find a &ldquo;better&rdquo; estimator.</li>
</ul>
<h3 id="properties-of-mles">Properties of MLEs</h3>
<ul>
<li>Under some regularity conditions, a MLE is consistent, asymptotically normal, and asymptotically efficient. In other words, when $n$ is large,</li>
</ul>
<p>$$
\hat{\theta}(X_1,\dots,X_n) \overset{\cdot}{\sim} N(\theta,I(\theta)^{-1})
$$</p>
<p>​	  <strong>Remark:</strong> this property is often referred to as an &ldquo;optimality of the MLE&rdquo;.</p>
<ul>
<li>A maximum likelihood estimator $\hat{\theta}(X_1,\dots,X_n)$ is always a function of a sufficient statistic.</li>
</ul>
<h2 id="sufficient-statistics-and-rao-blackwellization">Sufficient Statistics and Rao-Blackwellization</h2>
<h3 id="learning-objectives-4">Learning Objectives</h3>
<ul>
<li>
<p>Understand the concept of sufficient statistics</p>
</li>
<li>
<p>Understand how to find sufficient statistics</p>
</li>
<li>
<p>Understand how we can improve an unbiased estimator using a Rao-Blackwell theorem.</p>
</li>
</ul>
<h3 id="sufficient-statistics-and-factorization-theorem">Sufficient statistics and factorization theorem</h3>
<ul>
<li>
<p><strong>Definition (Sufficient Statistics)</strong> Let $X_1,\dots,X_n$ be a random sample from a probability distribution with unknown parameter $\theta$. Then the statistic $T(X_1,\dots,X_n)$ is said to be <strong>sufficient for $\theta$</strong> if the conditional distribution of $X_1,\dots,X_n$ given $T(X_1,\dots,X_n)$ does not depend on $\theta$.</p>
<p><strong>Remark:</strong> Roughly speaking, sufficiency of $T$ means that given the value of $T(X_1,\dots,X_n)$, we cannot gain any further information about $p$ by looking at other functions of $X_1,\dots,X_n$.</p>
<p><strong>Example</strong> Suppose we have a random sample $(X_1,\dots,X_n)$, when $X_i \sim {\rm Ber}(p)$. Show $Y= \sum_{i=1}^n X_i$ is sufficient for $p$.</p>
<blockquote>
<p>Compute the conditional pdf
$$
\begin{align}
p_{X_1,\dots,X_n|Y}(x_1,\dots,x_n|y)
&amp;= P(X_1=x_1,\dots,X_n = x_n |Y=y) \<br>
&amp;= \frac{P(X_1=x_1,\dots,X_n = x_n , Y=y)}{P(Y=y)}\<br>
&amp;= \begin{cases}
0 &amp; \mbox{ if } \sum_{i=1}^n x_i \neq y\<br>
\frac{p^{x_1}(1-p)^{1-x_1}\dots p^{x_n}(1-p)^{1-x_n} }{\binom{n}{y}p^y(1-p)^{n-y}} &amp; \mbox{ if } \sum_{i=1}^n x_i = y
\end{cases}\<br>
&amp; = \begin{cases}
0 &amp; \mbox{ if } \sum_{i=1}^n x_i \neq y\<br>
\frac{1 }{\binom{n}{y} } &amp; \mbox{ if } \sum_{i=1}^n x_i = y
\end{cases}
\end{align}
$$</p>
</blockquote>
<p>This definition tells us how to check whether a statistic is sufficient, but it does not tell us how to find a sufficient statistic. The following theorem is very useful in search of a sufficient statistic.</p>
</li>
<li>
<p><strong>Theorem (Factorization Theorem)</strong> Let $X_1,X_2,&hellip;,X_n$ denote random variables with joint pdf or pmf $f(x_1, x_2,&hellip;, x_n; \theta)$, which depends on the parameter $θ$. The statistic $T(X_1,X_2,&hellip;,X_n)$ is sufficient for $\theta$ if and only if
$$
f(x_1, \dots ,x_n; θ) = \phi[T(x_1, x_2,&hellip;, x_n); θ]h(x_1, x_2,&hellip;, x_n),
$$</p>
</li>
</ul>
<p>​		where $\phi$ depends on $x_1,x_2,\dots, x_n$ only through $T(x_1,&hellip;, x_n)$ and $h(x_1,&hellip;, x_n)$ does not depend on $\theta$.</p>
<p>​		<strong>Theorem (Factorization Theorem for two parameters)</strong> Let $X_1,X_2,&hellip;,X_n$ denote random variables with joint pdf or pmf $f(x_1, x_2,&hellip;, x_n; \theta_1,\theta_2)$, which depends on the parameter $\theta_1, \theta_2$. The statistics $T_1(X_1,X_2,&hellip;,X_n)$, $T_2(X_1,X_2,&hellip;,X_n)$ are (jointly) sufficient for $\theta_1,\theta_2$ if and only if
$$
f(x_1, \dots ,x_n; θ_1,θ_2) = \phi[T_1(x_1, x_2,&hellip;, x_n), T_2(x_1, x_2,&hellip;, x_n); θ_1,θ_2]h(x_1, x_2,&hellip;, x_n),
$$</p>
<p>​		where $\phi$ depends on $x_1,x_2,\dots, x_n$ only through $T_1(\cdot)$ and $T_2(\cdot)$, and $h(x_1,&hellip;, x_n)$ does not depend on $\theta_1, \theta_2$.</p>
<ul>
<li>
<p><strong>Example (one parameter):</strong>   Let $X_1,X_2,&hellip;,X_n$ be a random sample from a Poisson distribution with parameter $\lambda &gt;0$.</p>
<blockquote>
<p>$$
p(x_1,\dots,x_n; \lambda) = \prod_{i=1}^n \frac{e^{-\lambda}\lambda^{x_i}}{x_i!} = \frac{e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n x_i!}
$$</p>
<p>Therefore, $\sum_{i=1}^n X_i$ is sufficient for $\lambda$. In fact, $g(\sum_{i=1}^n X_i)$ is sufficient for any one-to-one $g$ (E.g. $\bar{X} = \sum_{i=1}^n X_i/n$).</p>
</blockquote>
</li>
<li>
<p><strong>Example (two parameters):</strong>   Let $X_1,X_2,&hellip;,X_n$ be a random sample from a Normal distribution with parameter $\mu,\sigma^2 &gt;0$</p>
<blockquote>
<p>$$
\begin{align}
f(x_1,\dots,x_n; \mu,\sigma^2)
&amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2} (x_i-\mu)^2) \<br>
&amp;= \prod_{i=1}^n\exp{-\frac{1}{2\sigma^2}x_i^2 + \frac{1}{\sigma^2} \mu x_i - \frac{1}{2\sigma^2}\mu^2 - \log\sqrt{2\pi\sigma^2} }\<br>
&amp;= \exp{-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 + \frac{1}{\sigma^2} \mu(\sum_{i=1}^n x_i) - \frac{n}{2\sigma^2}\mu^2 - n\log\sqrt{2\pi\sigma^2} }
\end{align}
$$</p>
<p>Therefore ($\sum_{i=1}^n X_i, \sum_{i=1}^n X_i^2$) are (jointly) sufficient for $(\mu,\sigma^2)$.</p>
</blockquote>
</li>
<li>
<p><strong>Theorem (Sufficient Statistic when a pmf/pdf is of the exponential form)</strong>  Let $X_1,X_2,&hellip;,X_n$ be a random sample from a distribution with a pdf or pmf $f(x; \theta)$ of the form
$$
f(x;\theta) = \exp {K(x) p(\theta) +S(x) + q(\theta)}
$$
on a support free of $\theta$. Then the statistic $\sum_{i=1}^n K(X_i)$ is sufficient for $\theta$.</p>
<blockquote>
<p>Proof. Factorization theorem.</p>
</blockquote>
</li>
<li>
<p><strong>Example:</strong>   Let $X_1,X_2,&hellip;,X_n$ be a random sample from a Poisson distribution with parameter $\lambda &gt;0$.</p>
<blockquote>
<p>$p(x;\lambda ) =  \frac{e^{-\lambda}\lambda^{x_i}}{x_i!} = \exp{-\lambda +x \log \lambda -\log x!}$</p>
<p>Therefore $K(x) = x$, and $\sum_{i=1}^n X_i$ is sufficient for $\lambda$.</p>
</blockquote>
</li>
</ul>
<h3 id="rao-blackwellization">Rao-Blackwellization</h3>
<p>Previously, we mentioned that a &ldquo;good&rdquo; estimator should be based on a sufficient statistic because a sufficient statistic contains <em>all</em> information about a parameter. In other words, given a sufficient statistic, adding an another statistic (i.e., a function of $X_1,\dots,X_n$) only introduces a noise, because there is no information left in the conditional distribution of $X_1,\dots,X_n$.</p>
<p>In fact, if we have an unbiased estimator which is not based on a sufficient statistic, we can <strong>improve</strong> the estimator based on the sufficient statistic.</p>
<p><strong>Theorem (Rao-Blackwell Theorem)</strong>  Let $X_1,X_2,&hellip;,X_n$ be a random sample from a distribution with pdf or pmf $f(x; \theta), \theta \in \Omega$.
Let $\hat{\theta}(X_1,X_2,&hellip;,X_n)$ be an unbiased estimator of $θ$. Let $T(X_1,X_2,&hellip;,X_n)$ be a sufficient statistic for $θ$, and $\hat{\theta}^* = E[\hat{\theta}(X_1,X_2,&hellip;,X_n)|T(X_1,\dots,X_n)]$. Then,</p>
<ol>
<li>$\hat{\theta}^<em>$ is a function of the sufficient statistic $T(X_1,\dots,X_n)$, where the function does not depend on $\theta$. In particular, $\widehat{\theta}^</em>$ a statistic.</li>
<li>$E[\hat{\theta}^*] = \theta$.</li>
<li>${\rm Var}(\hat{\theta}^*) \leq {\rm Var}(\hat{\theta})$, where the inequality is strict if the original estimator $\hat{\theta}(X_1,X_2,&hellip;,X_n)$ was not a function of $T(X_1,\dots,X_n)$ alone.</li>
</ol>
<p><strong>Remark1</strong> the new estimator $\hat{\theta}^*$, which is based on the sufficient statistic $T(X_1,\dots,X_n)$, is a better estimator than the original estimator if the original estimator $\hat{\theta}(X_1,X_2,&hellip;,X_n)$ was not a function of $T(X_1,\dots,X_n)$ alone.</p>
<p><strong>Remark 2</strong> In many cases, by improving an estimator based on the Rao-Blackwell theorem (called Rao-Blackwellization), we not only get an estimator with a smaller variance but we actually obtain a minimum-variance unbiased estimator (MVUE). This is especially true when we use a sufficient statistic  $\sum_{i=1}^n K(X_i)$ from the previous theorem (Sufficient Statistic when a pmf/pdf is of the exponential form).</p>

</div>

        </div>
    <div id="footer">
        <hr>
        <div class="container text-center mb-2">
            <a href="/"><small>© Copyright 2020 Hyebin Song</small></a>
        </div>
    </div>

</body>
</html>
